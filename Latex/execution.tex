\section{Experiment Execution}
Our experiment starts with the setup. Android Runner is executed on a laptop with Ubuntu 18.04.1 LTS, Intel i7-4702MQ and 8GB RAM. With the laptop connected to the Android devices via USB, all the runs will be executed from the laptop.The runs of the experiment are done on the same wifi network with a speed of 100 Mbps. To ensure that the wifi conditions do not alter the experiment output, the mobile device is always placed 5 meters from the router.

We picked the first 100 web apps from the top 1 million Alexa list. We used Lighthouse-batch to get the JSON files with the report of Lighthouse. This returns JSON files which contain the scores of the performance test. We enforce the simulation of 3G connectivity and we also force the simulation of first-time loads (ignoring the cache). This simulation delivers a full report: an aggregated score for each performance category, and also the raw values for each individual load time milestone, in addition to the settings and parameters that were requested for a given test run. The aggregate performance score is the relevant independent variable, thus it is extracted from these reports. 

From these 100 web apps, we randomly select a web app and fill it in a category: Good, Average, Poor, based on its score. This is done until each category reaches 7 web apps. If the randomly selected web app fits a category that already reached its limit of 7, it is discarded. The web apps are ready to use when a total of 21 subjects are selected

From the selected 21 web apps, we run the tests using Android runner which will execute each web app on Google Chrome. \hlcyan{We make the tests with a duration of 60 seconds on intervals of 2 minutes between each run. For the experiment consider the energy consumed up to the first-time-interactive metric given by lighthouse for each web app in order to not punish web apps that load in less than 60 seconds }.  We collect the energy readings using an automated script to execute 21 web apps for 25 trials each. On each run, Android runner will also start the Trepn profile tool and generate a csv file with the power consumed. 
 
Given the design of the experiment, we must take statistical tests for one factor and more than two treatments. The measurements are independent from each other, and energy consumption is a continuous function. We must then analyze  the distribution of these variables in order to check whether a parametric or non-parametric test will hold.  We start with identifying outliers by using a box plot. Furthermore, we intend to use Q-Q Plots with respect to the normal distribution in conjunction with the Shapiro - Wilks test to verify the normality of the dependent variable. We will also verify the normality of the residuals. We will then verify the whether or not an equal variance exists between all sample groups. The parametric checks may not hold initially. In some cases, transforming the data may fit the assumptions better. 

\hlcyan{Upon verifying these assumptions, we will perform either the one-way ANOVA test or Kruskal-Wallis on the collected data. This test will verify whether the means of the different performance score groups stem from unequal population variances. If indeed the population variances differ, we intend to perform Tukey's or Dunn's test to identify which scoring groups differ from each other.  Furthermore, we would like to understand the effect size if indeed an effect exists. This will be done by applying Cohen's d.}

\newpage