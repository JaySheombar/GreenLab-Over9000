cliff.delta( energies_with_ranks$Energy_consumption, energies_with_ranks$Performance_score, formula=Energy_consumption ~ Performance_score, data =energies_with_ranks)
cliff.delta( energies_with_ranks$Energy_consumption, energies_with_ranks$Performance_score, formula=Energy_consumption ~ Rank, data =energies_with_ranks)
cliff.delta( energies_with_ranks$Energy_consumption, energies_with_ranks$Performance_score, formula=Energy_consumption ~ Rank, data =energies_with_ranks)
cor.test(energies_with_ranks$Energy_consumption,energies_with_ranks$Rank, method="spearman")
cliff.delta( energies_with_ranks$Energy_consumption, energies_with_ranks$Performance_score, formula=Energy_consumption ~ Rank, data =energies_with_ranks)
cliff.delta( energies_with_ranks$Energy_consumption, energies_with_ranks$Performance_score)
cliff.delta( energies_with_ranks$Energy_consumption, energies_with_ranks$Rank)
cliff.delta( energies_with_ranks$Rank, energies_with_ranks$Energy_consumption)
cliff.delta( energies_with_ranks$Performance_score, energies_with_ranks$Energy_consumption)
cliff.delta( energies_with_ranks$Energy_consumption, energies_with_ranks$Performance_score, formula=Energy_consumption ~ Rank, data =energies_with_ranks)
library(dunn.test)
install.packages("dunn.test")
library(dunn.test)
dunn.test(x = energies_with_ranks$Energy_consumption, g = energies_with_ranks$Performance, method="bonferroni")
dunn.test(x = energies_with_ranks$Energy_consumption, g = energies_with_ranks$Performance, method="holm")
dunn.test(x = energies_with_ranks$Energy_consumption, g = energies_with_ranks$Performance, method="bonferroni")
dunn.test(x = energies_with_ranks$Energy_consumption, g = energies_with_ranks$Performance, method="holm")
dunnTest(energies_with_ranks$Energy_consumption, energies_with_ranks$Performance, method="Bonferroni")
install.packages("FSA")
library(FSA)
install.packages("FSA")
install.packages("FSA")
install.packages("FSA")
install.packages("FSA")
load.packages(FSA)
library(FSA)
library("FSA")
library("FSA")
dunn.test(x = energies_with_ranks$Energy_consumption, g = energies_with_ranks$Performance, method="holm")
library(ggplot2)
library(reshape)
library(e1071)
library(effsize)
library(dunn.test)
dunn.test(x = energies_with_ranks$Energy_consumption, g = energies_with_ranks$Performance, method="holm")
cor.test(energies_with_ranks$Rank,energies_with_ranks$Energy_consumption, method="spearman")
cliff.delta( energies_with_ranks$Energy_consumption, energies_with_ranks$Performance_score, formula=Energy_consumption ~ Rank, data =energies_with_ranks)
dunn.test(x = energies_with_ranks$Energy_consumption, g = energies_with_ranks$Performance, method="holm")
energy_good_performance <- energies_with_ranks[energies_with_ranks$Performance == "Good"]
energy_good_performance <- energies_with_ranks[energies_with_ranks$Rank == 3=]
energy_good_performance <- energies_with_ranks[energies_with_ranks$Rank == 3]
energy_good_performance <- energies_with_ranks[[energies_with_ranks$Rank == 3]]
energy_good_performance <- energies_with_ranks[energies_with_ranks$Rank == 3, ]
View(energy_good_performance)
View(energy_good_performance)
energy_good_performance <- energies_with_ranks[energies_with_ranks$Performance == "Good", ]
energy_average_performance <- energies_with_ranks[energies_with_ranks$Rank == "Average", ]
energy_poor_performance <- energies_with_ranks[energies_with_ranks$Rank == "Poor", ]
View(energy_poor_performance)
View(energy_poor_performance)
View(energy_poor_performance)
View(energy_good_performance)
View(energy_good_performance)
View(energy_consumption_values)
View(energy_average_performance)
View(energies_with_ranks)
View(energies_with_ranks)
energy_average_performance <- energies_with_ranks[energies_with_ranks$Rank == "Average", ]
energy_poor_performance <- energies_with_ranks[energies_with_ranks$Rank == "Poor", ]
View(energy_average_performance)
energy_poor_performance <- energies_with_ranks[energies_with_ranks$Performance == "Poor", ]
energy_average_performance <- energies_with_ranks[energies_with_ranks$Performance == "Average", ]
---
title: "Assignment 4"
author: "Group 9: C.M Valladares, K. Chan-Jong-Chu, S.Sheombar"
output: pdf_document
---
# Exercise 1
```{r echo=FALSE}
fruitflies = read.table("fruitflies.txt", header = TRUE)
fruitflies$loglongevity = log(fruitflies$longevity)
```
## Part 2
* Numerical outcome: loglongevity.
* Factor: activity.
* Numerical explanatory variable X: Thorax, it's measured and can not be controled
```{r echo=FALSE}
attach(fruitflies)
plot(loglongevity~thorax,pch=unclass(activity))
```
* High activity:  o
* Isolated activity: $\triangle$
* Low activity:  +
## Part 3
```{r echo=FALSE}
fruitflies$activity=as.factor(fruitflies$activity)
fruitfliesaov=lm(loglongevity~activity,data=fruitflies)
anova(fruitfliesaov)
```
The test for activity has a p-value of 1.798e-07, which shows that activity alone has a significant effect on the sexual activity.
## Part 4
```{r echo=FALSE}
summary(fruitfliesaov)
confint(fruitfliesaov)
```
* The intercept for base level-> high activity: $Y_{high}=3.60212 + error$
* $Y_{isolated}=3.60212+0.51722+e_{2}=4.11934 + error$
* $Y_{low}=3.60212+0.39771+e_{3}=3,99983 + error$
For these estimations, isolated and low activity will lead to an increase in longevity. Isolated will lead to the highest longevity. and high activty has the lowest longevity.
## Part 5
```{r echo=FALSE}
fruitfliesaov1=lm(loglongevity~thorax+activity,data=fruitflies)
anova(fruitfliesaov1)
```
Including Thorax shows that activity and thorax both have influence on the longevity because thorax is also a significant value.
## Part 6
```{r echo=FALSE}
contrasts(fruitflies$activity)=contr.sum
fruitfliesaov3=lm(loglongevity~thorax+activity,data=fruitflies)
summary(fruitfliesaov3)
average=mean(thorax)
small=min(thorax)
```
The linear ANCOVA model assumes that
Estimate for $Y_{i,n}=\mu+\alpha_{i}+\beta X_{i,n}+e_{i,n}$
* $\alpha_{1}=-0.23189$
* $\alpha_{2}= 0.17809$
* $\alpha_{3}=-\alpha_{1}-\alpha_{2}=0.0538$
* $\beta=2.97899$
* $\mu = 1.45083$
Using the average length $X=0.8245$:
* activty high->$Y_{i,n}=1.45083+-0.23189 + 2.97899* 0.8245+e_{i,n}$ = __3.675__ + error
* activty isolated->$Y_{i,n}=1.45083+0.17809+2.97899* 0.8245+e_{i,n}$ = __4.085__ + error
* activity low $Y_{i,n}=1.45083+0.0538+2.97899* 0.8245+e_{i,n}$ = __3.96__ + error
Using the min length $X=0.64$:
* activty high->$Y_{i,n}=1.45083+-0.23189+2.97899* 0.64+e_{i,n}$ = __3.125__ + error
* activty isolated->$Y_{i,n}=1.45083+0.17809+2.97899* 0.64+e_{i,n}$ = __3.535__ + error
* activity low $Y_{i,n}=1.45083+2.97899* 0.64+e_{i,n}$ = __3.41__ + error
Looking at the estimates. sexual activity decreases longevity if the activity is too high.
Isolated sexual activity leads to the longest longevity. Thorax plays a role in longevity as well. The average sized fly will have an increased longevity compared to small flies.
## Part 7
```{r echo=FALSE}
plot(loglongevity~thorax,pch=unclass(activity))
abline(lm(loglongevity~thorax,data=fruitflies[fruitflies$activity=='isolated',]))
abline(lm(loglongevity~thorax,data=fruitflies[fruitflies$activity=='low',]))
abline(lm(loglongevity~thorax,data=fruitflies[fruitflies$activity=='high',]))
```
And looking at the slope of the function, the bigger the thorax, the longevity increases linear.
There is no indication that the true lines ar not parallel so the dependence is similar under all three conditions of sexual activity.
## Part 8
Rereading the setting of our designed test: Adding the numerical explanatory variable thorax, which is a-priori evident will only increase the precision of our analysis. Looking at both tests, with and without adding thorax, sexual activity will have an influence on the longevity. If you're only interested in longevity, you can use the test without thorax. But for more precise analysis
## Part 9
```{r echo=FALSE}
qqnorm(residuals(fruitfliesaov1))
plot(fitted(fruitfliesaov1),residuals(fruitfliesaov1))
```
Looking at the qqnorm, we can assume normality because it shows a rather straight line. We can also assume equal population variances, because the spread in residuals does not change systematically with the fitted values.
## Part 10
```{r echo=FALSE}
aovdaysfireflies=lm(longevity~thorax+activity,data=fruitflies)
drop1(aovdaysfireflies,test="F")
qqnorm(residuals(aovdaysfireflies))
plot(fitted(aovdaysfireflies),residuals(aovdaysfireflies))
```
the qq plot shows a rather straight line as well, so we can assume normality.
For fitted plot, we see a pattern that increases with higher fitted values. So we would say that the logarithmic outcome was wise to use.
# Exercise 2
```{r echo=FALSE}
teaching = read.table("psi.txt", header = TRUE)
```
##Part 1
* Outcome Y is binary: passed or not
* Numerical explanatory variables X: gpa
* Factor explanatory variable: psi or not
```{r echo=FALSE}
attach(teaching)
qqnorm(gpa, main="Normal Q-Q Plot for GPA")
par(mfrow=c(1,2),oma=c(0,0,2,0))
boxplot(gpa~psi, xlab="PSI", ylab= "GPA")
boxplot(gpa~passed, xlab="Passed", ylab= "GPA")
title("GPA Distribution per Explanatory Variable", outer=TRUE)
teaching_xtab = xtabs(passed~psi+gpa, data=teaching); teaching_xtab
```
The x-tabs show the combination between the teaching method,traditional or psi, and the Student's GPA.  As it is shown, the outcome of the data is binary (pass or fail).
```{r echo=FALSE}
totgpa=xtabs(~gpa, data=teaching); totgpa
par(mfrow=c(1,2))
hist(teaching[,3],main='GPA')
barplot(xtabs(passed~gpa,data=teaching)/totgpa)
```
The bar graph shows the percentage of students that passed or failed depending on their gpa.
## Part 2
```{r echo=FALSE}
teaching$psi = as.factor(teaching$psi)
teaching$gpa = as.numeric(teaching$gpa)
teachingglm=glm(passed~gpa+psi,data=teaching,family=binomial);
summary(teachingglm)
```
## Part 3
```{r echo=FALSE}
summary(teachingglm)
plot(passed~gpa,pch=unclass(psi))
```
From the summary, we can see that employing PSI as a teaching method increases the linear predictor by 2.338. Therefore, the odds of passing increase by a factor of $e^{2.338}= 10.36$.  This means that PSI teaching does work.
## Part 4
The probability for a student with a gpa of 3 receiving PSI teaching is as follows: $Pr(Y_{psi,3.0})=\psi(\mu + \alpha_i + \beta X_{psi,3.0})$ where $\psi(x) = 1/(1 + e^{-x})$..
1.  $\alpha_{traditional}=-1.1689$
2.  $\alpha_{psi}= 1.1689$ because $\sum \alpha _i = 0$
3.  $\mu=-10.4327$
4.  $\beta=3.063$
Thus, $Pr(Y_{psi,3.0})= \psi (-10.4327 + (1.1689 ) + (3.063) * 3.0) = \dfrac{1}{1 + e^{-(-0.0748)}} = 0.4816$ or 48.1% chance for a person under a psi teaching model to pass
Under a traditional teachin model the odds are   $\psi (-10.4327 - (1.1689 ) + (3.063) * 3.0) = \dfrac{1}{1 + e^{-(-2.4126)}} = 0.08221692$ or a 8.2% chance.
## Part 5
An estimated change in odds from from using PSI and a traditional method multiplies the odds by $e^{\alpha_{PSI}-\alpha_{Traditional}}=e^{2.3378}= 10.35842$.  So if the odds of passing is k,  then the odds of passing by using PSI is $10.358*k$.  This can be interpreted as having odds of passing with psi being 10.358 times higher than the odds of passing with a traditional method.
```{r echo=FALSE}
teaching$psi=as.factor(teaching$psi)
teaching$gpa=as.factor(teaching$gpa)
teachingglm_fac=glm(passed~gpa+psi,data=teaching,family=binomial);
plot(c(0,coef(teachingglm_fac)))
```
From the graph for the coefficients for the gpa's we can see that no pattern is pressent between each level,  thus we can say that there is no dependence on GPA.
```{r echo=FALSE}
drop1(teachingglm_fac, test="Chisq")
```
Using the Drop1 command, one variable is deleted at a time from the model and we test the null hypothesis for each variable that all parameters attached to a given variable are 0.  Looking at the p-values from the test,  each variable cannot reject the null hypothesis, thus all parameters attached to psi are 0, hence it is not dependent on gpa.
## part 6
```{r echo=FALSE}
x=matrix(c(3,15,8,6),2,2); x
fisher.test(x)
```
This tests the null hypothesis that the probabilty of improving is the same for both groups. With a p-value of 0.0265, this hypothesis can be rejected. So there is a difference between the group that received psi and the group that did not.
## part 7
This second approach assumes that all students are equal and does not take the GPA in account.
For a more precise analysis we would add the GPA since it is significant.
## part 8
The advantage of the second method is to easily check if one factor has a proportional effect or not. The disadvantage for the second model is that we can check for specific effects.
The first method is more granular, for which you can test the effect of different variables, dependencies. The disadvantage for the first method is that our analaysis are highly dependent on the quality of our model.
# Exercise 3
## Part 1
```{r echo=FALSE}
africa = read.table('africa.txt', header = TRUE)
attach(africa)
par(mfrow=c(2,5))
hist(rpois(10, 0.5))
hist(rpois(10, 5))
hist(rpois(10, 15))
hist(rpois(10, 30))
hist(rpois(10, 100))
hist(rpois(50, 0.5))
hist(rpois(50, 5))
hist(rpois(50, 15))
hist(rpois(50, 30))
hist(rpois(50, 100))
hist(rpois(100, 0.5))
hist(rpois(100, 5))
hist(rpois(100, 15))
hist(rpois(100, 30))
hist(rpois(100, 100))
hist(rpois(1000, 0.5))
hist(rpois(1000, 5))
hist(rpois(1000, 15))
hist(rpois(1000, 30))
hist(rpois(1000, 100))
```
Looking at the different histograms we can observe that the larger our lambda value gets the more we approach normality (for set n values) for our poisson lambda distributions.
The other way around, keeping our lambda values equal and varying n we observe most histograms look equal except for the Y values being higher.
## Part 2
The mean and variance of the Poisson($\lambda$)-distribution both equal $\lambda$. For which in poisson regression is modelled as:
$log\lambda=\beta_0+\beta_1X_1+...\beta _pX_p$.
where the expression on the right indicates the combination of (numerical and/or categorical) explanatory variables.
For each observation $Y$ the parameter $\lambda$ is modelled differently, since the corresponding values of $X_1, . . . , X_p$ will differ in general. Hence, the variances in different observations are different as well. This means that residuals do not come from one fixed distribution. (A normal QQ-plot of these response residuals is not useful!). So they can not be investigated the same as for the normal distribution.
To investigate if different poisson distributions are from the same location scale family, the deviance residuals is usefull. Deviance is a measure of the discrepancy between the full model and the model under consideration. Deviance residuals are response residuals scaled by the deviance of that observation.
## Part 3
```{r echo=FALSE}
aglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim, family=poisson, data=africa)
summary(aglm)
```
Looking at this summary we can already say that there are alot of variables with a high p-value. Therefore we have a model which we can improve on removing them step by step, which we will do in part 4.
## Part 4
Here we do the step-down method:
```{r echo=FALSE}
summary(aglm)
```
Looking at the summary we find 'numelec' have a p value $>0.5$. It also is the highest p-value (0.80605) so we remove it.
```{r echo=FALSE}
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim, family=poisson, data=africa))
```
Looking at the summary we find 'numregim' have a p value $>0.5$. It also is the highest p-value (0.42639) so we remove it.
```{r echo=FALSE}
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size, family=poisson, data=africa))
```
Looking at the summary we find 'size' have a p value $>0.5$. It also is the highest p-value (0.421378) so we remove it.
```{r echo=FALSE}
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn, family=poisson, data=africa))
```
Looking at the summary we find 'popn' have a p value $>0.5$. It also is the highest p-value (0.29883) so we remove it.
```{r echo=FALSE}
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote, family=poisson, data=africa))
```
Looking at the summary we find 'pctvote' have a p value $>0.5$. It also is the highest p-value (0.13591) so we remove it.
```{r echo=FALSE}
summary(glm(miltcoup~oligarchy+pollib+parties, family=poisson, data=africa))
```
We finally end up with a model where all variables have a $p-value<0.05$. The explanatory variables for 'miltcoup' are 'oligarchy', 'pollib' and 'parties'.
## Part 5
### Model (with explanatory variables):
```{r echo=FALSE}
par(mfrow=c(1,3))
amodel = glm(miltcoup~oligarchy+pollib+parties, family=poisson, data=africa)
plot(fitted(amodel), residuals(amodel))
plot(log(fitted(amodel)), residuals(amodel))
plot(log(fitted(amodel)), residuals(amodel, type="response"))
```
### Full model (considering all variables):
```{r echo=FALSE}
par(mfrow=c(1,3))
plot(fitted(aglm), residuals(aglm))
plot(log(fitted(aglm)), residuals(aglm))
plot(log(fitted(aglm)), residuals(aglm, type="response"))
```
Looking at the plots we observe a pattern, however this pattern looks roughly the same for the full model (considering all variables).
Therefore we can say that is not due to deleting to many variables.
\newpage
# APPENDIX
## Exercise 1
```{r eval=FALSE}
fruitflies = read.table("fruitflies.txt", header = TRUE)
fruitflies$loglongevity = log(fruitflies$longevity)
```
### Part 2
```{r eval=FALSE}
attach(fruitflies)
plot(loglongevity~thorax,pch=unclass(activity))
```
### Part 3
```{r eval=FALSE}
fruitflies$activity=as.factor(fruitflies$activity)
fruitfliesaov=lm(loglongevity~activity,data=fruitflies)
anova(fruitfliesaov)
```
### Part 4
```{r eval=FALSE}
summary(fruitfliesaov)
confint(fruitfliesaov)
```
## Part 5
```{r eval=FALSE}
fruitfliesaov1=lm(loglongevity~thorax+activity,data=fruitflies)
anova(fruitfliesaov1)
```
## Part 6
```{r eval=FALSE}
contrasts(fruitflies$activity)=contr.sum
fruitfliesaov3=lm(loglongevity~thorax+activity,data=fruitflies)
summary(fruitfliesaov3)
mean(thorax)
min(thorax)
```
## Part 7
```{r eval=FALSE}
plot(loglongevity~thorax,pch=unclass(activity))
abline(lm(loglongevity~thorax,data=fruitflies[fruitflies$activity=='isolated',]))
abline(lm(loglongevity~thorax,data=fruitflies[fruitflies$activity=='low',]))
abline(lm(loglongevity~thorax,data=fruitflies[fruitflies$activity=='high',]))
```
### Part 9
```{r eval=FALSE}
qqnorm(residuals(fruitfliesaov1))
plot(fitted(fruitfliesaov1),residuals(fruitfliesaov1))
```
### Part 10
```{r eval=FALSE}
aovdaysfireflies=lm(longevity~thorax+activity,data=fruitflies)
qqnorm(residuals(aovdaysfireflies))
plot(fitted(aovdaysfireflies),residuals(aovdaysfireflies))
```
## Excercise 2
```{r eval=FALSE}
teaching = read.table("psi.txt", header = TRUE)
```
###Part 1
```{r eval=FALSE}
attach(teaching)
par(mfrow=c(1,2))
boxplot(gpa~psi, xlab="PSI", ylab= "GPA")
boxplot(gpa~passed, xlab="Passed", ylab= "GPA")
teaching_xtab = xtabs(passed~psi+gpa, data=teaching); teaching_xtab
```
### Part 2
```{r eval=FALSE}
teaching$psi = as.factor(teaching$psi)
teaching$gpa = as.numeric(teaching$gpa)
teachingglm=glm(passed~gpa+psi,data=teaching,family=binomial);
summary(teachingglm)
```
### Part 3
```{r eval=FALSE}
summary(teachingglm)
plot(passed~gpa,pch=unclass(psi))
```
### Part 5
```{r eval=FALSE}
teaching$psi=as.factor(teaching$psi)
teaching$gpa=as.factor(teaching$gpa)
teachingglm_fac=glm(passed~gpa+psi,data=teaching,family=binomial);
plot(c(0,coef(teachingglm_fac)))
```
### Part 6
```{r eval=FALSE}
x=matrix(c(3,15,8,6),2,2); x
fisher.test(x)
```
## Exercise 3
### Part 1
```{r eval=FALSE}
d = read.table('africa.txt', header = TRUE)
par(mfrow=c(2,5))
hist(rpois(10, 0.5))
hist(rpois(10, 5))
hist(rpois(10, 15))
hist(rpois(10, 30))
hist(rpois(10, 100))
hist(rpois(50, 0.5))
hist(rpois(50, 5))
hist(rpois(50, 15))
hist(rpois(50, 30))
hist(rpois(50, 100))
hist(rpois(100, 0.5))
hist(rpois(100, 5))
hist(rpois(100, 15))
hist(rpois(100, 30))
hist(rpois(100, 100))
hist(rpois(1000, 0.5))
hist(rpois(1000, 5))
hist(rpois(1000, 15))
hist(rpois(1000, 30))
hist(rpois(1000, 100))
```
### Part 3
```{r eval=FALSE}
aglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim,
family=poisson, data=d)
summary(aglm)
```
### Part 4
```{r eval=FALSE}
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim,
family=poisson, data=d))
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim,
family=poisson, data=d))
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size, family=poisson, data=d))
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn, family=poisson, data=d))
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote, family=poisson, data=d))
summary(glm(miltcoup~oligarchy+pollib+parties, family=poisson, data=d))
```
### Part 5
```{r eval=FALSE}
amodel = glm(miltcoup~oligarchy+pollib+parties, family=poisson, data=d)
plot(fitted(amodel), residuals(amodel))
plot(log(fitted(amodel)), residuals(amodel))
plot(log(fitted(amodel)), residuals(amodel, type="response"))
aglm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim,
family=poisson, data=d)
plot(fitted(aglm), residuals(aglm))
plot(log(fitted(aglm)), residuals(aglm))
plot(log(fitted(aglm)), residuals(aglm, type="response"))
```
summary(glm(miltcoup~oligarchy+pollib+parties+pctvote+popn, family=poisson, data=d))
fruitflies = read.table("fruitflies.txt", header = TRUE)
fruitflies$loglongevity = log(fruitflies$longevity)
attach(fruitflies)
plot(loglongevity~thorax,pch=unclass(activity))
fruitflies = read.table("fruitflies.txt", header = TRUE)
fruitflies$loglongevity = log(fruitflies$longevity)
fruitflies$activity=as.factor(fruitflies$activity)
fruitfliesaov=lm(loglongevity~activity,data=fruitflies)
anova(fruitfliesaov)
summary(fruitfliesaov)
confint(fruitfliesaov)
fruitfliesaov1=lm(loglongevity~thorax+activity,data=fruitflies)
anova(fruitfliesaov1)
contrasts(fruitflies$activity)=contr.sum
fruitfliesaov3=lm(loglongevity~thorax+activity,data=fruitflies)
summary(fruitfliesaov3)
average=mean(thorax)
small=min(thorax)
plot(loglongevity~thorax,pch=unclass(activity))
abline(lm(loglongevity~thorax,data=fruitflies[fruitflies$activity=='isolated',]))
abline(lm(loglongevity~thorax,data=fruitflies[fruitflies$activity=='low',]))
abline(lm(loglongevity~thorax,data=fruitflies[fruitflies$activity=='high',]))
qqnorm(residuals(fruitfliesaov1))
plot(fitted(fruitfliesaov1),residuals(fruitfliesaov1))
aovdaysfireflies=lm(longevity~thorax+activity,data=fruitflies)
drop1(aovdaysfireflies,test="F")
qqnorm(residuals(aovdaysfireflies))
plot(fitted(aovdaysfireflies),residuals(aovdaysfireflies))
attach(teaching)
totgpa=xtabs(~gpa, data=teaching); totgpa
teaching$psi = as.factor(teaching$psi)
pairs(energies_with_ranks$Energy_consumption~energies_with_ranks$Performance, data=energies_with_ranks)
cor.test(energy_good_performance, energy_average_performance)
cor.test(energy_good_performance,energy_average_performance, method="spearman")
View(energy_average_performance)
# Make an analysis of Correlion which is non-parametric
cor.test(energies_with_ranks$Rank,energies_with_ranks$Energy_consumption, method="spearman")
cor.test(energies_with_ranks$Energy_consumption,energies_with_ranks$Performance_score, method="spearman")
cor.test(energies_with_ranks$Energy_consumption,energies_with_ranks$Rank, method="spearman")
# Make an analysis of Correlion which is non-parametric.
#   Perform it between the ranks and the Energy COnsumption (Ranks must be ordered
#   Such that high performance has a higher value than lower performance)
cor.test(energies_with_ranks$Energy_consumption,energies_with_ranks$Rank, method="spearman")
# Make an analysis of Correlion which is non-parametric.
#   Perform it between the ranks and the Energy COnsumption (Ranks must be ordered
#   Such that high performance has a higher value than lower performance)
cor.test(energies_with_ranks$Energy_consumption,energies_with_ranks$Rank, method="spearman")
dunn.test(x = energies_with_ranks$Energy_consumption, g = energies_with_ranks$Performance, method="holm", kw=TRUE)
dunn.test(x = energies_with_ranks$Energy_consumption, g = energies_with_ranks$Performance, method="holm")
library(FSA)
install.packages("FSA")
install.packages("FSA")
install.packages("FSA")
install.packages("FSA")
